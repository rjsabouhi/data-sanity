
Tool #4: data-sanity: A tiny dataset health & anomaly checker

A 1-command library that inspects a CSV and returns a human-readable “health report.”

Why this one?
	•	Data engineers love fast validations.
	•	Machine learning people love quality checks.
	•	Analysts love simple, readable outputs.
	•	Nobody wants to install Great Expectations (too heavy).
	•	This hits an extremely painful, everyday problem.

The pitch is:

Datasanity: a tiny (<200 loc) CSV health checker that reports missing-data %, duplicates, type drift, cardinality explosions, and basic anomaly scores.

Super easy. Super useful. Zero dependencies outside numpy + pandas.

⸻

FILES YOU WILL HAND TO REPLIT

Everything below is ready to copy/paste.
This is the entire functioning package.

⸻

PROJECT STRUCTURE

datasanity/
    datasanity/
        __init__.py
        core.py
        report.py
    tests/
        test_core.py
    demo.csv
    run_demo.py
    pyproject.toml
    README.md
    LICENSE


⸻

core.py

import pandas as pd
import numpy as np

def analyze_csv(path):
    df = pd.read_csv(path)

    report = {}

    # 1. Shape
    report["rows"] = len(df)
    report["columns"] = df.columns.tolist()

    # 2. Missing values
    report["missing_pct"] = (df.isna().mean() * 100).round(2).to_dict()

    # 3. Duplicate rows
    report["duplicate_rows"] = int(df.duplicated().sum())

    # 4. Cardinality per column
    report["cardinality"] = {col: df[col].nunique() for col in df.columns}

    # 5. Basic anomaly score (per column z-score > 3)
    anomaly_counts = {}
    numeric_df = df.select_dtypes(include=[np.number])
    for col in numeric_df.columns:
        z = (numeric_df[col] - numeric_df[col].mean()) / (numeric_df[col].std() + 1e-9)
        anomaly_counts[col] = int((np.abs(z) > 3).sum())

    report["anomalies"] = anomaly_counts

    return report


⸻

report.py

def pretty_report(report):
    lines = []
    lines.append("=== DataSanity Report ===")
    lines.append(f"Rows: {report['rows']}")
    lines.append("Columns: " + ", ".join(report["columns"]))
    lines.append("")

    lines.append("Missing %:")
    for col, pct in report["missing_pct"].items():
        lines.append(f"  {col}: {pct}%")

    lines.append("\nDuplicate rows: " + str(report["duplicate_rows"]))

    lines.append("\nCardinality:")
    for col, card in report["cardinality"].items():
        lines.append(f"  {col}: {card}")

    lines.append("\nAnomalies (>3σ):")
    for col, count in report["anomalies"].items():
        lines.append(f"  {col}: {count}")

    return "\n".join(lines)


⸻

__init__.py

from .core import analyze_csv
from .report import pretty_report

__all__ = ["analyze_csv", "pretty_report"]


⸻

test_core.py

from datasanity import analyze_csv

def test_basic():
    import pandas as pd
    df = pd.DataFrame({"x": [1,2,3,4], "y":[10,20,30,40]})
    df.to_csv("tmp.csv", index=False)

    r = analyze_csv("tmp.csv")
    assert r["rows"] == 4
    assert "missing_pct" in r


⸻

run_demo.py

from datasanity import analyze_csv, pretty_report

result = analyze_csv("demo.csv")
print(pretty_report(result))


⸻

pyproject.toml

[project]
name = "datasanity"
version = "0.1.0"
description = "A tiny (<200 loc) CSV health + anomaly detector"
authors = [{ name="RJ Sabouhi" }]
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.8"
dependencies = ["pandas", "numpy"]

[project.urls]
Homepage = "https://github.com/rjsabouhi/datasanity"
Repository = "https://github.com/rjsabouhi/datasanity"


⸻

README.md

# datasanity

A tiny (<200 lines) CSV health-check + anomaly-detection tool.

It reports:

- Missing value percentages  
- Duplicate row count  
- Cardinality per column  
- Basic anomaly score (z-score > 3)  
- Row + column summary  

## Install

```bash
pip install datasanity

Usage

from datasanity import analyze_csv, pretty_report

r = analyze_csv("data.csv")
print(pretty_report(r))

CLI Demo

python run_demo.py

---

# `LICENSE`
MIT (same as your other packages)

---

# Demo File (`demo.csv`)
```csv
value,score
10,100
11,102
12,104
13,500
14,108